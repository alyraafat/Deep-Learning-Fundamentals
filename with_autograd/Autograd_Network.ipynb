{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6dbf533",
   "metadata": {},
   "source": [
    "[https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f8f79f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708ddd0b-501f-42e4-a03e-fd788308df67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 22:05:48.051316: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-22 22:05:48.225933: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-22 22:05:48.225974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-22 22:05:48.255042: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-22 22:05:48.314827: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 22:05:49.040850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import keras\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "\n",
    "from optimizers.sgd import SGD\n",
    "from optimizers.adam import Adam\n",
    "# from network import Network\n",
    "# from layers.conv2d import Conv2D\n",
    "# from layers.dense import FCLayer\n",
    "# from layers.activation import ActivationLayer, SoftmaxLayer, tanh, tanh_prime, softmax, softmax_prime, relu, relu_prime, sigmoid, sigmoid_prime\n",
    "# from layers.flatten import FlattenLayer\n",
    "from losses import mse, categorical_crossentropy, binary_crossentropy, mae\n",
    "from layers import Conv2D, FCLayer, ActivationLayer, FlattenLayer, DropoutLayer, PoolingLayer, GlobalAveragePoolingLayer, RNN, MaxPool2D\n",
    "from autograd import tanh, relu, sigmoid, softmax, Module, Tensor\n",
    "from utils import draw_computation_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9218fe3a-33d6-4120-9707-25d1eb7eeb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_test_samples(x_test,y_test,samples,network):\n",
    "  for test, true in zip(x_test[:samples], y_test[:samples]):\n",
    "    pred = network.predict([test])[0][0]\n",
    "    idx = np.argmax(pred)\n",
    "    idx_true = np.argmax(true)\n",
    "    plt.title('pred: %s, prob: %.2f, true: %d' % (idx, pred[idx], idx_true))\n",
    "    plt.imshow(test, cmap='binary')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e4645e-3125-4c92-883e-d40beba683c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_data(image,is_conv=False):\n",
    "  image = np.cast['float32'](image)\n",
    "  if is_conv:\n",
    "    image = np.expand_dims(image,axis=-1)\n",
    "  image/=255\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8820af65-b125-4121-a2f4-ac6c1c524acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a23092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = adjust_data(x_train,is_conv=True)\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "x_test = adjust_data(x_test,is_conv=True)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dda0d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab5b76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_batches(data, labels, batch_size):\n",
    "#     num_batches = int(len(data) / batch_size)\n",
    "#     data_batches = []\n",
    "#     label_batches = []\n",
    "    \n",
    "#     for i in range(num_batches):\n",
    "#         start = i * batch_size\n",
    "#         end = min((i + 1) * batch_size, len(data))\n",
    "#         data_batches.append(data[start:end])\n",
    "#         label_batches.append(labels[start:end])\n",
    "    \n",
    "#     return np.array(data_batches), np.array(label_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "000bf977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# x_train_batches, y_train_batches = create_batches(x_train, y_train, batch_size)\n",
    "# x_test_batches, y_test_batches = create_batches(x_test, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf3e2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_batches.shape, y_train_batches.shape, x_test_batches.shape, y_test_batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1a546a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_batches = adjust_data(x_train_batches,True)\n",
    "# y_train_batches = keras.utils.to_categorical(y_train_batches)\n",
    "# x_test_batches = adjust_data(x_test_batches,True)\n",
    "# y_test_batches = keras.utils.to_categorical(y_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d33dba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_batches.shape, y_train_batches.shape, x_test_batches.shape, y_test_batches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6a045-36d6-444b-bf5f-00cf08b1607a",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fe2d775-4861-4597-93ab-e4595d53226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2048.0813414951094\n",
      "1 1707.7629585846835\n",
      "2 1553.6914227549987\n",
      "3 1448.9663620719796\n",
      "4 1381.9981000480261\n",
      "5 1309.722369372564\n",
      "6 1254.9350448028144\n",
      "7 1213.7559963481597\n",
      "8 1173.7931039650628\n",
      "9 1137.6871342093411\n",
      "10 1110.7935089069817\n",
      "11 1092.8674974686126\n",
      "12 1073.8671768510292\n",
      "13 1058.3378667344193\n",
      "14 1045.5894116737684\n",
      "15 1027.979069781539\n",
      "16 1016.6574633903596\n",
      "17 1006.551125824417\n",
      "18 997.4419170206762\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m inputs \u001b[38;5;241m=\u001b[39m x_train[start:end]\n\u001b[0;32m---> 40\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m actual \u001b[38;5;241m=\u001b[39m y_train[start:end]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# print(predicted.shape, actual.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/autograd/module.py:32\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Tensor, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m     12\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x1, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/layers/layer.py:21\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, inp, training)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_initialized:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(inp\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/layers/conv2d.py:88\u001b[0m, in \u001b[0;36mConv2D.forward_propagation\u001b[0;34m(self, inp, training)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_channels):\n\u001b[0;32m---> 88\u001b[0m         cross_corr_out \u001b[38;5;241m=\u001b[39m \u001b[43mcross_correlation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_conv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m         intermediate_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_conv[b,f] \u001b[38;5;241m+\u001b[39m cross_corr_out\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_conv\u001b[38;5;241m.\u001b[39mset_item((b,f),intermediate_res)\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/layers/conv2d.py:30\u001b[0m, in \u001b[0;36mcross_correlation\u001b[0;34m(inp, kernel, padding)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(new_height):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(new_width):\n\u001b[0;32m---> 30\u001b[0m         mat \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj\u001b[49m\u001b[43m:\u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# output[i,j] = np.sum(mat*kernel)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         convolve \u001b[38;5;241m=\u001b[39m (mat\u001b[38;5;241m*\u001b[39mkernel)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/autograd/tensor.py:132\u001b[0m, in \u001b[0;36mTensor.__getitem__\u001b[0;34m(self, idxs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idxs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensor\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _slice\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midxs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/autograd/ops.py:148\u001b[0m, in \u001b[0;36m_slice\u001b[0;34m(x, idxs)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m zeros_grad\n\u001b[1;32m    143\u001b[0m     depends_on\u001b[38;5;241m.\u001b[39mappend(Dependency(tensor\u001b[38;5;241m=\u001b[39mx, grad_fn\u001b[38;5;241m=\u001b[39mgrad_fn))\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Tensor(\n\u001b[1;32m    145\u001b[0m     data\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdata[idxs],\n\u001b[1;32m    146\u001b[0m     requires_grad\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mrequires_grad,\n\u001b[1;32m    147\u001b[0m     depends_on\u001b[38;5;241m=\u001b[39mdepends_on,\n\u001b[0;32m--> 148\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_slice_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midxs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    149\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Model(Module):\n",
    "    def __init__(self) -> None:\n",
    "        self.conv = Conv2D(filters=3, kernel_size=5, padding='valid')\n",
    "        self.maxpool = MaxPool2D(pool_size=2, strides=1)\n",
    "        self.flatten = FlattenLayer()\n",
    "        self.linear1 = FCLayer(output_dim=20)\n",
    "        self.linear2 = FCLayer(output_dim=10)\n",
    "\n",
    "    def forward(self, inputs: Tensor, training=True) -> Tensor:\n",
    "        x = self.conv(inputs, training=training)\n",
    "        x1 = self.maxpool(x)\n",
    "        x2 = self.flatten(x1, training=training)\n",
    "        x3 = self.linear1(x2, training=training)\n",
    "        x4 = tanh(x3)\n",
    "        x5 = self.linear2(x4, training=training)\n",
    "        x6 = tanh(x5)\n",
    "        x7 = softmax(x6)\n",
    "        return x7\n",
    "\n",
    "optimizer = SGD(learning_rate=0.001)\n",
    "batch_size = 32\n",
    "model = Model()\n",
    "x_train = Tensor(x_train)\n",
    "y_train = Tensor(y_train)\n",
    "i = 0\n",
    "# print(x_train.shape)\n",
    "# train_data = x_train.shape[0]\n",
    "train_data = 1000\n",
    "\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0.0\n",
    "    # print(f'before: {x_train.shape}')\n",
    "    for start in range(0, train_data, batch_size):\n",
    "        end = start + batch_size\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        inputs = x_train[start:end]\n",
    "\n",
    "        predicted = model(inputs)\n",
    "        actual = y_train[start:end]\n",
    "        # print(predicted.shape, actual.shape)\n",
    "        loss = categorical_crossentropy(y_true=actual, y_pred=predicted)\n",
    "        if i==0:\n",
    "            draw_computation_graph(loss)\n",
    "        i+=1\n",
    "        # for param in model.parameters():\n",
    "        #     print(param.name, param.id)\n",
    "        # print(start, loss)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.data\n",
    "\n",
    "        optimizer.step(model)\n",
    "    # print(f'after: {x_train.shape}')\n",
    "\n",
    "    print(epoch, epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de14286",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fdfa810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 3, 1) (800,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def create_dataset(num_sequences, sequence_length):\n",
    "    return np.array([np.arange(start, start + sequence_length) for start in range(num_sequences)])\n",
    "\n",
    "# Helper function to create inputs and targets from the sequences\n",
    "def create_inputs_targets(data):\n",
    "    X = data[:, :-1]  # all but the last item in each sequence as input\n",
    "    Y = data[:, -1]  # all but the first item in each sequence as target (for sequence prediction)\n",
    "    return X, Y\n",
    "\n",
    "# Helper function to create batches\n",
    "def create_batches(X, Y, batch_size):\n",
    "    num_batches = len(X) // batch_size\n",
    "    X_batches = np.array(np.array_split(X, num_batches))\n",
    "    Y_batches = np.array(np.array_split(Y, num_batches))\n",
    "    return X_batches, Y_batches\n",
    "\n",
    "# Generate dataset\n",
    "num_sequences = 1000  # The number of sequences you want\n",
    "sequence_length = 4  # The length of each sequence\n",
    "batch_size = 32  # The size of each batch\n",
    "\n",
    "# Create dataset\n",
    "dataset = create_dataset(num_sequences, sequence_length)\n",
    "\n",
    "# Split dataset into training and testing sets (80-20 split)\n",
    "train_size = int(num_sequences * 0.8)\n",
    "train_set, test_set = dataset[:train_size], dataset[train_size:]\n",
    "\n",
    "# Create inputs (X) and targets (Y) for training and testing\n",
    "x_train, y_train = create_inputs_targets(train_set)\n",
    "x_test, y_test = create_inputs_targets(test_set)\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "# Shuffle training data\n",
    "indices = np.arange(train_size)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Apply shuffled indices to create shuffled training data\n",
    "x_train_shuffled = x_train[indices]\n",
    "y_train_shuffled = y_train[indices]\n",
    "# Normalize inputs\n",
    "# x_train_max = np.max(x_train_shuffled)\n",
    "# x_train_shuffled = x_train_shuffled / x_train_max\n",
    "# x_test = x_test / x_train_max  # use the same scale as train set\n",
    "\n",
    "# print(x_train_shuffled.shape, y_train_shuffled.shape)\n",
    "# Create batches from the training and testing data\n",
    "# x_train_batches, y_train_batches = create_batches(x_train_shuffled, y_train_shuffled, batch_size)\n",
    "# x_test_batches, y_test_batches = create_batches(x_test, y_test, batch_size)\n",
    "\n",
    "\n",
    "# Example: Print the first training batch\n",
    "# print(\"First training batch (x_train, y_train):\")\n",
    "# print(x_train_batches.shape, y_train_batches.shape)\n",
    "# x_train_batches[0], y_train_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c8288a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3071105.4242313574\n",
      "1 1519365.3534559796\n",
      "2 1335884.3266659623\n",
      "3 1314244.880436437\n",
      "4 1311661.7313077284\n",
      "5 1311292.4955469666\n",
      "6 1311170.1587656126\n",
      "7 1311071.7958194683\n",
      "8 1310974.5203389414\n",
      "9 1310876.839759497\n",
      "10 1310778.997427537\n",
      "11 1310681.166784061\n",
      "12 1310583.4183357004\n",
      "13 1310485.7774524984\n",
      "14 1310388.2527725673\n",
      "15 1310290.8474389527\n",
      "16 1310193.5620489758\n",
      "17 1310096.3972843909\n",
      "18 1309999.352734552\n",
      "19 1309902.4284628998\n",
      "20 1309805.6242607355\n",
      "21 1309708.9399760473\n",
      "22 1309612.3755748917\n",
      "23 1309515.930724284\n",
      "24 1309419.6054442017\n",
      "25 1309323.399480707\n",
      "26 1309227.3126449508\n",
      "27 1309131.3448757564\n",
      "28 1309035.4960095019\n",
      "29 1308939.7659313332\n",
      "30 1308844.1543197772\n",
      "31 1308748.6611739653\n",
      "32 1308653.286266676\n",
      "33 1308558.0295468494\n",
      "34 1308462.8907765357\n",
      "35 1308367.8698288063\n",
      "36 1308272.966540985\n",
      "37 1308178.1806952825\n",
      "38 1308083.5122791436\n",
      "39 1307988.9610949385\n",
      "40 1307894.5269820578\n",
      "41 1307800.209751961\n",
      "42 1307706.0092684915\n",
      "43 1307611.9254625526\n",
      "44 1307517.9580850687\n",
      "45 1307424.1070095159\n",
      "46 1307330.372145188\n",
      "47 1307236.7532308546\n",
      "48 1307143.2502732635\n",
      "49 1307049.8629891349\n",
      "50 1306956.5913551147\n",
      "51 1306863.4350401303\n",
      "52 1306770.394076088\n",
      "53 1306677.4681808592\n",
      "54 1306584.6573405208\n",
      "55 1306491.9612625497\n",
      "56 1306399.3799027572\n",
      "57 1306306.9131670038\n",
      "58 1306214.5608444852\n",
      "59 1306122.322647829\n",
      "60 1306030.19861231\n",
      "61 1305938.1885163356\n",
      "62 1305846.2922160088\n",
      "63 1305754.5095815961\n",
      "64 1305662.8405594693\n",
      "65 1305571.2848346676\n",
      "66 1305479.8423629936\n",
      "67 1305388.512927437\n",
      "68 1305297.296468326\n",
      "69 1305206.1928259067\n",
      "70 1305115.2018165905\n",
      "71 1305024.3232755382\n",
      "72 1304933.5571328024\n",
      "73 1304842.9032557374\n",
      "74 1304752.3613980426\n",
      "75 1304661.931514653\n",
      "76 1304571.6133141147\n",
      "77 1304481.4068471356\n",
      "78 1304391.3118810176\n",
      "79 1304301.3282636807\n",
      "80 1304211.4558942807\n",
      "81 1304121.694549034\n",
      "82 1304032.0441395442\n",
      "83 1303942.504587906\n",
      "84 1303853.075628913\n",
      "85 1303763.757131919\n",
      "86 1303674.5490696563\n",
      "87 1303585.4511911008\n",
      "88 1303496.4634562326\n",
      "89 1303407.5856323652\n",
      "90 1303318.8175383725\n",
      "91 1303230.1591729946\n",
      "92 1303141.610352683\n",
      "93 1303053.1709338697\n",
      "94 1302964.8406559434\n",
      "95 1302876.6195291618\n",
      "96 1302788.5073210772\n",
      "97 1302700.5039526818\n",
      "98 1302612.6093430477\n",
      "99 1302524.8232065896\n",
      "100 1302437.1454929013\n",
      "101 1302349.5760363876\n",
      "102 1302262.1146935946\n",
      "103 1302174.761376891\n",
      "104 1302087.5158149232\n",
      "105 1302000.3780708406\n",
      "106 1301913.3478031654\n",
      "107 1301826.4250765357\n",
      "108 1301739.6096118588\n",
      "109 1301652.9013314825\n",
      "110 1301566.3000102157\n",
      "111 1301479.8055665586\n",
      "112 1301393.4179804008\n",
      "113 1301307.1368983784\n",
      "114 1301220.9623610005\n",
      "115 1301134.8941136897\n",
      "116 1301048.9320776488\n",
      "117 1300963.0761249766\n",
      "118 1300877.3261235165\n",
      "119 1300791.6818846904\n",
      "120 1300706.1433583642\n",
      "121 1300620.7102918292\n",
      "122 1300535.3826352765\n",
      "123 1300450.1602620722\n",
      "124 1300365.0429668508\n",
      "125 1300280.0306963385\n",
      "126 1300195.123280761\n",
      "127 1300110.3206062468\n",
      "128 1300025.622445279\n",
      "129 1299941.0287358412\n",
      "130 1299856.5394573568\n",
      "131 1299772.154247797\n",
      "132 1299687.8730738198\n",
      "133 1299603.695922889\n",
      "134 1299519.6224743442\n",
      "135 1299435.652619376\n",
      "136 1299351.7864085431\n",
      "137 1299268.023546706\n",
      "138 1299184.3638894635\n",
      "139 1299100.8074175247\n",
      "140 1299017.3539607825\n",
      "141 1298934.0032208639\n",
      "142 1298850.7553150582\n",
      "143 1298767.6100265821\n",
      "144 1298684.5670841765\n",
      "145 1298601.6265875648\n",
      "146 1298518.7882463292\n",
      "147 1298436.0519654474\n",
      "148 1298353.417642336\n",
      "149 1298270.8851357624\n",
      "150 1298188.4542155757\n",
      "151 1298106.124950966\n",
      "152 1298023.8970280734\n",
      "153 1297941.7704882566\n",
      "154 1297859.744953294\n",
      "155 1297777.8206425582\n",
      "156 1297695.9971348685\n",
      "157 1297614.27437311\n",
      "158 1297532.6522818827\n",
      "159 1297451.1306995312\n",
      "160 1297369.709489611\n",
      "161 1297288.388511724\n",
      "162 1297207.167706781\n",
      "163 1297126.046885897\n",
      "164 1297045.0259124597\n",
      "165 1296964.10468632\n",
      "166 1296883.2831315098\n",
      "167 1296802.5609735325\n",
      "168 1296721.938240943\n",
      "169 1296641.41466997\n",
      "170 1296560.9902895633\n",
      "171 1296480.6648900274\n",
      "172 1296400.4383127724\n",
      "173 1296320.3104182743\n",
      "174 1296240.2812056143\n",
      "175 1296160.3504111844\n",
      "176 1296080.5179883929\n",
      "177 1296000.783783717\n",
      "178 1295921.1476867704\n",
      "179 1295841.609501321\n",
      "180 1295762.1692322388\n",
      "181 1295682.8266147948\n",
      "182 1295603.5816953569\n",
      "183 1295524.4341063849\n",
      "184 1295445.3839352536\n",
      "185 1295366.4310150298\n",
      "186 1295287.5750990815\n",
      "187 1295208.8162363118\n",
      "188 1295130.1542072422\n",
      "189 1295051.5888540903\n",
      "190 1294973.120211987\n",
      "191 1294894.7479324928\n",
      "192 1294816.4720246068\n",
      "193 1294738.2924268458\n",
      "194 1294660.2088203072\n",
      "195 1294582.2212479617\n",
      "196 1294504.3295945823\n",
      "197 1294426.5336895885\n",
      "198 1294348.8332192765\n",
      "199 1294271.2283764705\n",
      "200 1294193.7189243766\n",
      "201 1294116.3046798692\n",
      "202 1294038.985524183\n",
      "203 1293961.7614482192\n",
      "204 1293884.632211345\n",
      "205 1293807.5977758442\n",
      "206 1293730.6578831738\n",
      "207 1293653.8125823464\n",
      "208 1293577.0617390985\n",
      "209 1293500.405086509\n",
      "210 1293423.8425976192\n",
      "211 1293347.3742093104\n",
      "212 1293270.9996476213\n",
      "213 1293194.7189548083\n",
      "214 1293118.5319148793\n",
      "215 1293042.4383476276\n",
      "216 1292966.4383728558\n",
      "217 1292890.5315669002\n",
      "218 1292814.7180885081\n",
      "219 1292738.9975421925\n",
      "220 1292663.3700862813\n",
      "221 1292587.8353884728\n",
      "222 1292512.393475479\n",
      "223 1292437.0441266834\n",
      "224 1292361.7873063725\n",
      "225 1292286.6227463144\n",
      "226 1292211.550539548\n",
      "227 1292136.570351019\n",
      "228 1292061.6822657792\n",
      "229 1291986.8860755572\n",
      "230 1291912.1816674215\n",
      "231 1291837.5688292333\n",
      "232 1291763.0475956376\n",
      "233 1291688.6177253018\n",
      "234 1291614.2793108989\n",
      "235 1291540.0319625589\n",
      "236 1291465.8757211047\n",
      "237 1291391.8104473138\n",
      "238 1291317.8360070826\n",
      "239 1291243.9522581375\n",
      "240 1291170.1591733224\n",
      "241 1291096.4565633107\n",
      "242 1291022.8443113589\n",
      "243 1290949.3223193137\n",
      "244 1290875.89046542\n",
      "245 1290802.548646842\n",
      "246 1290729.2967392951\n",
      "247 1290656.134657001\n",
      "248 1290583.0622468486\n",
      "249 1290510.0793747206\n",
      "250 1290437.1859768673\n",
      "251 1290364.3819801959\n",
      "252 1290291.6672047304\n",
      "253 1290219.041349193\n",
      "254 1290146.5046673836\n",
      "255 1290074.0568741397\n",
      "256 1290001.6978699046\n",
      "257 1289929.427409173\n",
      "258 1289857.245560059\n",
      "259 1289785.1521491199\n",
      "260 1289713.1470891857\n",
      "261 1289641.2301408376\n",
      "262 1289569.4013444101\n",
      "263 1289497.6604762739\n",
      "264 1289426.0075405734\n",
      "265 1289354.4423354282\n",
      "266 1289282.9647197386\n",
      "267 1289211.5747098315\n",
      "268 1289140.2720930644\n",
      "269 1289069.0567442167\n",
      "270 1288997.9286331353\n",
      "271 1288926.8876266428\n",
      "272 1288855.9335496407\n",
      "273 1288785.0663347542\n",
      "274 1288714.2858434329\n",
      "275 1288643.5920367448\n",
      "276 1288572.984783578\n",
      "277 1288502.4638647358\n",
      "278 1288432.02933197\n",
      "279 1288361.6809330191\n",
      "280 1288291.41860267\n",
      "281 1288221.2422968862\n",
      "282 1288151.1517880128\n",
      "283 1288081.1470723934\n",
      "284 1288011.228026211\n",
      "285 1287941.3945115206\n",
      "286 1287871.6464261294\n",
      "287 1287801.9835902115\n",
      "288 1287732.4060396412\n",
      "289 1287662.913570139\n",
      "290 1287593.5060455967\n",
      "291 1287524.1834256118\n",
      "292 1287454.945611278\n",
      "293 1287385.7924280087\n",
      "294 1287316.7238073703\n",
      "295 1287247.739663316\n",
      "296 1287178.8397917633\n",
      "297 1287110.0241992816\n",
      "298 1287041.2927675867\n",
      "299 1286972.6453025166\n",
      "300 1286904.081819849\n",
      "301 1286835.60204883\n",
      "302 1286767.20596169\n",
      "303 1286698.893479161\n",
      "304 1286630.6644550576\n",
      "305 1286562.5188430906\n",
      "306 1286494.4565104125\n",
      "307 1286426.4773281137\n",
      "308 1286358.5811559556\n",
      "309 1286290.7680128368\n",
      "310 1286223.037688791\n",
      "311 1286155.3899713156\n",
      "312 1286087.8250623\n",
      "313 1286020.342634753\n",
      "314 1285952.9425039166\n",
      "315 1285885.624855178\n",
      "316 1285818.3892878485\n",
      "317 1285751.2358738354\n",
      "318 1285684.1644255635\n",
      "319 1285617.174895574\n",
      "320 1285550.267181486\n",
      "321 1285483.4411518862\n",
      "322 1285416.6965614995\n",
      "323 1285350.033575505\n",
      "324 1285283.4519846032\n",
      "325 1285216.9515904102\n",
      "326 1285150.5323426232\n",
      "327 1285084.1941910347\n",
      "328 1285017.937050304\n",
      "329 1284951.7607188758\n",
      "330 1284885.6650191615\n",
      "331 1284819.6500437409\n",
      "332 1284753.715690291\n",
      "333 1284687.8616528946\n",
      "334 1284622.0880364904\n",
      "335 1284556.3945736787\n",
      "336 1284490.7813120645\n",
      "337 1284425.2480112803\n",
      "338 1284359.7947222984\n",
      "339 1284294.4211989108\n",
      "340 1284229.1273992988\n",
      "341 1284163.9132806105\n",
      "342 1284098.7785582056\n",
      "343 1284033.7233360969\n",
      "344 1283968.7473883647\n",
      "345 1283903.8506579634\n",
      "346 1283839.033090905\n",
      "347 1283774.294455081\n",
      "348 1283709.6348176415\n",
      "349 1283645.0538391715\n",
      "350 1283580.5517192658\n",
      "351 1283516.128145371\n",
      "352 1283451.7830230242\n",
      "353 1283387.5164370195\n",
      "354 1283323.32801572\n",
      "355 1283259.2178897453\n",
      "356 1283195.1858214717\n",
      "357 1283131.2318084922\n",
      "358 1283067.3556029072\n",
      "359 1283003.5572693576\n",
      "360 1282939.8366682187\n",
      "361 1282876.1936665499\n",
      "362 1282812.628054521\n",
      "363 1282749.1400192739\n",
      "364 1282685.7292072175\n",
      "365 1282622.3956116852\n",
      "366 1282559.139076902\n",
      "367 1282495.9596204183\n",
      "368 1282432.8571476121\n",
      "369 1282369.8313191354\n",
      "370 1282306.882312291\n",
      "371 1282244.0098345992\n",
      "372 1282181.2139746428\n",
      "373 1282118.4945387398\n",
      "374 1282055.851385386\n",
      "375 1281993.2844921853\n",
      "376 1281930.7936560179\n",
      "377 1281868.3788971477\n",
      "378 1281806.0401997573\n",
      "379 1281743.7771732816\n",
      "380 1281681.5899480716\n",
      "381 1281619.478382409\n",
      "382 1281557.44226575\n",
      "383 1281495.481724238\n",
      "384 1281433.5964454368\n",
      "385 1281371.7865223573\n",
      "386 1281310.0517121023\n",
      "387 1281248.3920111083\n",
      "388 1281186.8071974758\n",
      "389 1281125.2972648623\n",
      "390 1281063.8621649747\n",
      "391 1281002.5017243521\n",
      "392 1280941.2158146005\n",
      "393 1280880.0045034424\n",
      "394 1280818.8675533112\n",
      "395 1280757.804825214\n",
      "396 1280696.8164559605\n",
      "397 1280635.90215519\n",
      "398 1280575.0618097875\n",
      "399 1280514.2953740803\n",
      "400 1280453.6027596842\n",
      "401 1280392.9839211856\n",
      "402 1280332.4387765846\n",
      "403 1280271.9671169363\n",
      "404 1280211.5689111797\n",
      "405 1280151.2440296714\n",
      "406 1280090.9924101958\n",
      "407 1280030.814032225\n",
      "408 1279970.708669954\n",
      "409 1279910.6763575547\n",
      "410 1279850.7168581781\n",
      "411 1279790.8301463823\n",
      "412 1279731.016214435\n",
      "413 1279671.2748066324\n",
      "414 1279611.6059887565\n",
      "415 1279552.0095616477\n",
      "416 1279492.4855490841\n",
      "417 1279433.0335809193\n",
      "418 1279373.6539227264\n",
      "419 1279314.3463032793\n",
      "420 1279255.1105757514\n",
      "421 1279195.9467916833\n",
      "422 1279136.8547685915\n",
      "423 1279077.834411895\n",
      "424 1279018.8856391062\n",
      "425 1278960.0083940513\n",
      "426 1278901.2025145944\n",
      "427 1278842.46801287\n",
      "428 1278783.8047258318\n",
      "429 1278725.2125613664\n",
      "430 1278666.6914409401\n",
      "431 1278608.2413136654\n",
      "432 1278549.862046507\n",
      "433 1278491.5535612623\n",
      "434 1278433.315776958\n",
      "435 1278375.1485190352\n",
      "436 1278317.0518190195\n",
      "437 1278259.0254941597\n",
      "438 1278201.0695427603\n",
      "439 1278143.1837708063\n",
      "440 1278085.3681518906\n",
      "441 1278027.6225737694\n",
      "442 1277969.9470688403\n",
      "443 1277912.3413891082\n",
      "444 1277854.8055030685\n",
      "445 1277797.3391829976\n",
      "446 1277739.9425635682\n",
      "447 1277682.6155063747\n",
      "448 1277625.3578591975\n",
      "449 1277568.1694869902\n",
      "450 1277511.050368287\n",
      "451 1277454.0005592736\n",
      "452 1277397.0196881336\n",
      "453 1277340.107794637\n",
      "454 1277283.264864049\n",
      "455 1277226.490758014\n",
      "456 1277169.7853454507\n",
      "457 1277113.148564892\n",
      "458 1277056.5803399943\n",
      "459 1277000.0805692698\n",
      "460 1276943.649137317\n",
      "461 1276887.285979265\n",
      "462 1276830.9911441891\n",
      "463 1276774.7643356838\n",
      "464 1276718.6055814347\n",
      "465 1276662.5147268963\n",
      "466 1276606.4917328504\n",
      "467 1276550.5364989953\n",
      "468 1276494.6490177733\n",
      "469 1276438.828931454\n",
      "470 1276383.0764882076\n",
      "471 1276327.3915105066\n",
      "472 1276271.7738045005\n",
      "473 1276216.22327629\n",
      "474 1276160.7399860104\n",
      "475 1276105.3237693734\n",
      "476 1276049.974519909\n",
      "477 1275994.6920927358\n",
      "478 1275939.476619827\n",
      "479 1275884.32781022\n",
      "480 1275829.2456497424\n",
      "481 1275774.2300999146\n",
      "482 1275719.2809366605\n",
      "483 1275664.398192687\n",
      "484 1275609.5818577213\n",
      "485 1275554.8316403816\n",
      "486 1275500.1475749984\n",
      "487 1275445.5295340195\n",
      "488 1275390.9774486006\n",
      "489 1275336.4913444114\n",
      "490 1275282.0709444399\n",
      "491 1275227.7162793013\n",
      "492 1275173.4272999868\n",
      "493 1275119.2037403176\n",
      "494 1275065.045792594\n",
      "495 1275010.9531351447\n",
      "496 1274956.9257860298\n",
      "497 1274902.9636516208\n",
      "498 1274849.0667080544\n",
      "499 1274795.2346987496\n",
      "500 1274741.4677362298\n",
      "501 1274687.765605674\n",
      "502 1274634.128277851\n",
      "503 1274580.555705301\n",
      "504 1274527.047711812\n",
      "505 1274473.6043057893\n",
      "506 1274420.2253595707\n",
      "507 1274366.9107959198\n",
      "508 1274313.6604608567\n",
      "509 1274260.4744972615\n",
      "510 1274207.352534351\n",
      "511 1274154.2946983387\n",
      "512 1274101.3007729042\n",
      "513 1274048.370818406\n",
      "514 1273995.5046284355\n",
      "515 1273942.7021428896\n",
      "516 1273889.9634008887\n",
      "517 1273837.2880701078\n",
      "518 1273784.6763044056\n",
      "519 1273732.1279781421\n",
      "520 1273679.6429203933\n",
      "521 1273627.221143903\n",
      "522 1273574.8624786607\n",
      "523 1273522.5669133868\n",
      "524 1273470.3343405921\n",
      "525 1273418.164736011\n",
      "526 1273366.0578843094\n",
      "527 1273314.0138582345\n",
      "528 1273262.0325211799\n",
      "529 1273210.1136849336\n",
      "530 1273158.25748227\n",
      "531 1273106.4636454266\n",
      "532 1273054.7321654363\n",
      "533 1273003.0629900086\n",
      "534 1272951.4560008734\n",
      "535 1272899.9111055546\n",
      "536 1272848.4282466776\n",
      "537 1272797.0074689735\n",
      "538 1272745.6485026234\n",
      "539 1272694.3512391285\n",
      "540 1272643.1158030564\n",
      "541 1272591.942033925\n",
      "542 1272540.8296773667\n",
      "543 1272489.7789086394\n",
      "544 1272438.7896275406\n",
      "545 1272387.8615768014\n",
      "546 1272336.9947559924\n",
      "547 1272286.189168116\n",
      "548 1272235.4446574978\n",
      "549 1272184.7612397203\n",
      "550 1272134.1386317331\n",
      "551 1272083.576927696\n",
      "552 1272033.0759766784\n",
      "553 1271982.6357916\n",
      "554 1271932.2563094199\n",
      "555 1271881.9372272822\n",
      "556 1271831.678621579\n",
      "557 1271781.4804603928\n",
      "558 1271731.3426283335\n",
      "559 1271681.2650753993\n",
      "560 1271631.2476194636\n",
      "561 1271581.290235598\n",
      "562 1271531.3928740595\n",
      "563 1271481.555493065\n",
      "564 1271431.7779496207\n",
      "565 1271382.060174507\n",
      "566 1271332.4021706723\n",
      "567 1271282.803738324\n",
      "568 1271233.2648646613\n",
      "569 1271183.7853915768\n",
      "570 1271134.3654080853\n",
      "571 1271085.0046977266\n",
      "572 1271035.7032681608\n",
      "573 1270986.4610131087\n",
      "574 1270937.2778852326\n",
      "575 1270888.1537373129\n",
      "576 1270839.0886114617\n",
      "577 1270790.082212913\n",
      "578 1270741.1347183762\n",
      "579 1270692.2458871433\n",
      "580 1270643.4157277045\n",
      "581 1270594.6441586274\n",
      "582 1270545.930991806\n",
      "583 1270497.2763710902\n",
      "584 1270448.680066009\n",
      "585 1270400.1420046038\n",
      "586 1270351.6621161825\n",
      "587 1270303.240399199\n",
      "588 1270254.8767154044\n",
      "589 1270206.5709903466\n",
      "590 1270158.3232483696\n",
      "591 1270110.1332325966\n",
      "592 1270062.0009983245\n",
      "593 1270013.9264503915\n",
      "594 1269965.9094679344\n",
      "595 1269917.9500781733\n",
      "596 1269870.0481120031\n",
      "597 1269822.2035127666\n",
      "598 1269774.4163001315\n",
      "599 1269726.6862732843\n",
      "600 1269679.013407099\n",
      "601 1269631.3976061353\n",
      "602 1269583.8389219227\n",
      "603 1269536.3371376316\n",
      "604 1269488.8921969887\n",
      "605 1269441.504074795\n",
      "606 1269394.1726507023\n",
      "607 1269346.8978995553\n",
      "608 1269299.6797562162\n",
      "609 1269252.5181157459\n",
      "610 1269205.4129422323\n",
      "611 1269158.3640996993\n",
      "612 1269111.3715351822\n",
      "613 1269064.435280354\n",
      "614 1269017.5550605122\n",
      "615 1268970.73101144\n",
      "616 1268923.9629235126\n",
      "617 1268877.2507945276\n",
      "618 1268830.5945126996\n",
      "619 1268783.993996708\n",
      "620 1268737.4492501733\n",
      "621 1268690.9601789452\n",
      "622 1268644.5266322515\n",
      "623 1268598.1485971757\n",
      "624 1268551.825979029\n",
      "625 1268505.5587836865\n",
      "626 1268459.3469058461\n",
      "627 1268413.1901971875\n",
      "628 1268367.088645033\n",
      "629 1268321.0422211387\n",
      "630 1268275.0507725417\n",
      "631 1268229.1143072175\n",
      "632 1268183.232684783\n",
      "633 1268137.4058728416\n",
      "634 1268091.633799682\n",
      "635 1268045.9164374927\n",
      "636 1268000.2536226204\n",
      "637 1267954.645301292\n",
      "638 1267909.09140604\n",
      "639 1267863.5920739758\n",
      "640 1267818.1469170505\n",
      "641 1267772.755987897\n",
      "642 1267727.4193313564\n",
      "643 1267682.1367053497\n",
      "644 1267636.9081594772\n",
      "645 1267591.7335880455\n",
      "646 1267546.6128608345\n",
      "647 1267501.545967082\n",
      "648 1267456.532915615\n",
      "649 1267411.5734729813\n",
      "650 1267366.667735649\n",
      "651 1267321.8155344722\n",
      "652 1267277.0167125103\n",
      "653 1267232.2714866532\n",
      "654 1267187.5795260132\n",
      "655 1267142.9409238142\n",
      "656 1267098.3554583334\n",
      "657 1267053.8230880303\n",
      "658 1267009.343919933\n",
      "659 1266964.9176530687\n",
      "660 1266920.544429494\n",
      "661 1266876.2240667322\n",
      "662 1266831.9564866133\n",
      "663 1266787.7416354096\n",
      "664 1266743.579476336\n",
      "665 1266699.4699166196\n",
      "666 1266655.4129999366\n",
      "667 1266611.4084009812\n",
      "668 1266567.4563338442\n",
      "669 1266523.5565715127\n",
      "670 1266479.7090498982\n",
      "671 1266435.9137690943\n",
      "672 1266392.1705756856\n",
      "673 1266348.4794791832\n",
      "674 1266304.840444519\n",
      "675 1266261.2532360666\n",
      "676 1266217.7180374588\n",
      "677 1266174.2345683048\n",
      "678 1266130.8028476778\n",
      "679 1266087.4227538807\n",
      "680 1266044.0943756748\n",
      "681 1266000.817470292\n",
      "682 1265957.5920633166\n",
      "683 1265914.4180772775\n",
      "684 1265871.2954180138\n",
      "685 1265828.2241146083\n",
      "686 1265785.2038931714\n",
      "687 1265742.2349625193\n",
      "688 1265699.3170287726\n",
      "689 1265656.4501053113\n",
      "690 1265613.634170871\n",
      "691 1265570.869159274\n",
      "692 1265528.1548661964\n",
      "693 1265485.4914591806\n",
      "694 1265442.8786282083\n",
      "695 1265400.316535341\n",
      "696 1265357.8049469464\n",
      "697 1265315.3439030775\n",
      "698 1265272.9332391517\n",
      "699 1265230.5730016194\n",
      "700 1265188.2630044313\n",
      "701 1265146.0033327078\n",
      "702 1265103.7937932538\n",
      "703 1265061.6343833709\n",
      "704 1265019.5250260036\n",
      "705 1264977.4656450015\n",
      "706 1264935.4561835597\n",
      "707 1264893.4965222194\n",
      "708 1264851.5867760961\n",
      "709 1264809.726713835\n",
      "710 1264767.9163047387\n",
      "711 1264726.1555090472\n",
      "712 1264684.444289414\n",
      "713 1264642.7824948453\n",
      "714 1264601.1701631555\n",
      "715 1264559.6071633408\n",
      "716 1264518.09344709\n",
      "717 1264476.628980301\n",
      "718 1264435.21372197\n",
      "719 1264393.8474295377\n",
      "720 1264352.530214023\n",
      "721 1264311.2620885356\n",
      "722 1264270.0428039748\n",
      "723 1264228.872376241\n",
      "724 1264187.7507305099\n",
      "725 1264146.6778307767\n",
      "726 1264105.6535464132\n",
      "727 1264064.6779357828\n",
      "728 1264023.750837207\n",
      "729 1263982.8722072921\n",
      "730 1263942.0419596261\n",
      "731 1263901.2601094851\n",
      "732 1263860.5265336751\n",
      "733 1263819.8412573289\n",
      "734 1263779.2041069951\n",
      "735 1263738.6150440068\n",
      "736 1263698.0740129014\n",
      "737 1263657.5810438609\n",
      "738 1263617.1359461462\n",
      "739 1263576.73865329\n",
      "740 1263536.3892756253\n",
      "741 1263496.087534041\n",
      "742 1263455.8335576872\n",
      "743 1263415.6271788024\n",
      "744 1263375.4683712125\n",
      "745 1263335.357024303\n",
      "746 1263295.2930759343\n",
      "747 1263255.276591585\n",
      "748 1263215.3073932305\n",
      "749 1263175.3854758427\n",
      "750 1263135.5107734012\n",
      "751 1263095.6831024499\n",
      "752 1263055.902611476\n",
      "753 1263016.1690809245\n",
      "754 1262976.4825554197\n",
      "755 1262936.8428052748\n",
      "756 1262897.249961988\n",
      "757 1262857.7039897444\n",
      "758 1262818.2046020704\n",
      "759 1262778.751939884\n",
      "760 1262739.3458038531\n",
      "761 1262699.986217413\n",
      "762 1262660.673220236\n",
      "763 1262621.4065465431\n",
      "764 1262582.1862492978\n",
      "765 1262543.012227701\n",
      "766 1262503.8844649082\n",
      "767 1262464.8029054815\n",
      "768 1262425.7674357165\n",
      "769 1262386.7780938223\n",
      "770 1262347.8347008044\n",
      "771 1262308.9372528195\n",
      "772 1262270.0857052624\n",
      "773 1262231.2799840937\n",
      "774 1262192.5200088832\n",
      "775 1262153.805844329\n",
      "776 1262115.1372960515\n",
      "777 1262076.5143272136\n",
      "778 1262037.9368947388\n",
      "779 1261999.404860527\n",
      "780 1261960.9183797876\n",
      "781 1261922.4771823098\n",
      "782 1261884.0813122012\n",
      "783 1261845.730740349\n",
      "784 1261807.4252958968\n",
      "785 1261769.1649802173\n",
      "786 1261730.9497477033\n",
      "787 1261692.7795488664\n",
      "788 1261654.654329946\n",
      "789 1261616.5739419896\n",
      "790 1261578.5385729633\n",
      "791 1261540.5478419235\n",
      "792 1261502.6018804214\n",
      "793 1261464.7005734954\n",
      "794 1261426.8438487623\n",
      "795 1261389.0318176192\n",
      "796 1261351.2642344062\n",
      "797 1261313.5410092704\n",
      "798 1261275.862212206\n",
      "799 1261238.2278459677\n",
      "800 1261200.6376418641\n",
      "801 1261163.0916759784\n",
      "802 1261125.5899015502\n",
      "803 1261088.1322628837\n",
      "804 1261050.7186409938\n",
      "805 1261013.3489667587\n",
      "806 1260976.0232783463\n",
      "807 1260938.741507973\n",
      "808 1260901.5035540168\n",
      "809 1260864.3092969102\n",
      "810 1260827.1587348713\n",
      "811 1260790.0519407145\n",
      "812 1260752.9887119252\n",
      "813 1260715.9690370522\n",
      "814 1260678.992897758\n",
      "815 1260642.0601386481\n",
      "816 1260605.1707622663\n",
      "817 1260568.3247514605\n",
      "818 1260531.5219747191\n",
      "819 1260494.7623975305\n",
      "820 1260458.0460547262\n",
      "821 1260421.372747219\n",
      "822 1260384.7425004805\n",
      "823 1260348.1553179622\n",
      "824 1260311.6109860442\n",
      "825 1260275.1095343563\n",
      "826 1260238.6510088043\n",
      "827 1260202.2352058827\n",
      "828 1260165.8620817545\n",
      "829 1260129.5317625315\n",
      "830 1260093.2439450806\n",
      "831 1260056.9987281663\n",
      "832 1260020.7959332336\n",
      "833 1259984.6356350195\n",
      "834 1259948.5178372273\n",
      "835 1259912.4422534117\n",
      "836 1259876.4090458034\n",
      "837 1259840.418015288\n",
      "838 1259804.4691785672\n",
      "839 1259768.5624489102\n",
      "840 1259732.6978768401\n",
      "841 1259696.8752497805\n",
      "842 1259661.0945816769\n",
      "843 1259625.355880211\n",
      "844 1259589.6589104994\n",
      "845 1259554.003897327\n",
      "846 1259518.3905537874\n",
      "847 1259482.8190008306\n",
      "848 1259447.2889967514\n",
      "849 1259411.800607147\n",
      "850 1259376.3537435161\n",
      "851 1259340.9484371976\n",
      "852 1259305.5844884976\n",
      "853 1259270.2619608794\n",
      "854 1259234.9807491566\n",
      "855 1259199.7408350953\n",
      "856 1259164.542073914\n",
      "857 1259129.3845928253\n",
      "858 1259094.2680952006\n",
      "859 1259059.1927999302\n",
      "860 1259024.158444705\n",
      "861 1258989.1650235096\n",
      "862 1258954.2125803821\n",
      "863 1258919.300970597\n",
      "864 1258884.430191964\n",
      "865 1258849.600043073\n",
      "866 1258814.8107212745\n",
      "867 1258780.0621093092\n",
      "868 1258745.3539281606\n",
      "869 1258710.68633947\n",
      "870 1258676.0592488484\n",
      "871 1258641.4726481664\n",
      "872 1258606.9264122567\n",
      "873 1258572.4205154194\n",
      "874 1258537.9549113293\n",
      "875 1258503.5294488752\n",
      "876 1258469.1443023814\n",
      "877 1258434.7992872768\n",
      "878 1258400.4942497846\n",
      "879 1258366.2293032834\n",
      "880 1258332.0043913447\n",
      "881 1258297.8192644645\n",
      "882 1258263.6741611257\n",
      "883 1258229.5688608517\n",
      "884 1258195.503224057\n",
      "885 1258161.477457601\n",
      "886 1258127.4912569944\n",
      "887 1258093.5446913247\n",
      "888 1258059.6378028397\n",
      "889 1258025.7703157165\n",
      "890 1257991.9424070118\n",
      "891 1257958.1538742809\n",
      "892 1257924.4046988965\n",
      "893 1257890.694889944\n",
      "894 1257857.0242779246\n",
      "895 1257823.3929065745\n",
      "896 1257789.8007909986\n",
      "897 1257756.2477137914\n",
      "898 1257722.7337479952\n",
      "899 1257689.2587433665\n",
      "900 1257655.8227794715\n",
      "901 1257622.4257010247\n",
      "902 1257589.0674966818\n",
      "903 1257555.7481376114\n",
      "904 1257522.4675672424\n",
      "905 1257489.2257007193\n",
      "906 1257456.0225027064\n",
      "907 1257422.857935205\n",
      "908 1257389.7319746227\n",
      "909 1257356.6445581338\n",
      "910 1257323.5955095915\n",
      "911 1257290.5849675094\n",
      "912 1257257.6127528346\n",
      "913 1257224.6789911827\n",
      "914 1257191.7834339577\n",
      "915 1257158.9261206137\n",
      "916 1257126.1069990376\n",
      "917 1257093.3259944739\n",
      "918 1257060.583028255\n",
      "919 1257027.8782057932\n",
      "920 1256995.211295126\n",
      "921 1256962.5824228986\n",
      "922 1256929.9913901861\n",
      "923 1256897.4381787695\n",
      "924 1256864.9227704431\n",
      "925 1256832.4451524473\n",
      "926 1256800.0052199692\n",
      "927 1256767.6029416926\n",
      "928 1256735.238235671\n",
      "929 1256702.9110861006\n",
      "930 1256670.621565246\n",
      "931 1256638.3693413185\n",
      "932 1256606.1547007768\n",
      "933 1256573.9772796137\n",
      "934 1256541.837203211\n",
      "935 1256509.7344435914\n",
      "936 1256477.668923661\n",
      "937 1256445.640516373\n",
      "938 1256413.649245963\n",
      "939 1256381.6951490869\n",
      "940 1256349.7780029979\n",
      "941 1256317.8978757227\n",
      "942 1256286.0546908577\n",
      "943 1256254.2483353466\n",
      "944 1256222.4788903093\n",
      "945 1256190.7462164047\n",
      "946 1256159.050308315\n",
      "947 1256127.3911588944\n",
      "948 1256095.7685716318\n",
      "949 1256064.1826715174\n",
      "950 1256032.6333287589\n",
      "951 1256001.1204632507\n",
      "952 1255969.6441153255\n",
      "953 1255938.2041424445\n",
      "954 1255906.8005165092\n",
      "955 1255875.433379243\n",
      "956 1255844.1023766834\n",
      "957 1255812.8076371136\n",
      "958 1255781.5491701327\n",
      "959 1255750.3267969843\n",
      "960 1255719.1405381581\n",
      "961 1255687.9903468192\n",
      "962 1255656.8761318144\n",
      "963 1255625.7978563025\n",
      "964 1255594.7556113044\n",
      "965 1255563.7491317894\n",
      "966 1255532.7785842137\n",
      "967 1255501.843751974\n",
      "968 1255470.9446165168\n",
      "969 1255440.0811930092\n",
      "970 1255409.2534602077\n",
      "971 1255378.4613183078\n",
      "972 1255347.7046998974\n",
      "973 1255316.983574151\n",
      "974 1255286.2979652889\n",
      "975 1255255.6477613274\n",
      "976 1255225.0329776744\n",
      "977 1255194.453417191\n",
      "978 1255163.9091747946\n",
      "979 1255133.4002281046\n",
      "980 1255102.9264123468\n",
      "981 1255072.4877612258\n",
      "982 1255042.084203678\n",
      "983 1255011.7157846517\n",
      "984 1254981.3822964171\n",
      "985 1254951.0837822012\n",
      "986 1254920.820209164\n",
      "987 1254890.5915429727\n",
      "988 1254860.3976743873\n",
      "989 1254830.2386377056\n",
      "990 1254800.114321763\n",
      "991 1254770.0247146\n",
      "992 1254739.9697820153\n",
      "993 1254709.9494217576\n",
      "994 1254679.9636896912\n",
      "995 1254650.0124622819\n",
      "996 1254620.095662011\n",
      "997 1254590.2133883089\n",
      "998 1254560.365490869\n",
      "999 1254530.5519122952\n"
     ]
    }
   ],
   "source": [
    "# net_conv_adam.add(RNN(hidden_size=20,output_size=30, return_sequences=False))\n",
    "\n",
    "# net_conv_adam.add(FCLayer(output_dim=50))\n",
    "# net_conv_adam.add(FCLayer(output_dim=1))\n",
    "# net_conv_adam.add(SoftmaxLayer())\n",
    "\n",
    "# train on 1000 samples\n",
    "# we didn't implemented mini-batch GD\n",
    "# net_conv_adam.use(mse, mse_prime)\n",
    "# net_conv_adam.fit(x_train=x_train, y_train=y_train, epochs=100)\n",
    "class Model(Module):\n",
    "    def __init__(self) -> None:\n",
    "        self.rnn = RNN(hidden_size=20,output_size=30, return_sequences=False, bidirectional=False, init='glorot_uniform')\n",
    "        # self.rnn2 = RNN(hidden_size=20,output_size=30, return_sequences=False, init='glorot_uniform')\n",
    "        self.flatten = FlattenLayer()\n",
    "        # self.linear1 = FCLayer(output_dim=20)\n",
    "        self.linear2 = FCLayer(output_dim=1)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        x = self.rnn(inputs)\n",
    "        # x = self.rnn2(x)\n",
    "        x2 = self.flatten(x)\n",
    "        # x3 = self.linear1(x2)\n",
    "        x4 = self.linear2(x2)\n",
    "        return x4\n",
    "\n",
    "optimizer = SGD(learning_rate=0.001)\n",
    "batch_size = 32\n",
    "model = Model()\n",
    "x_train = Tensor(x_train_shuffled)\n",
    "y_train = Tensor(np.expand_dims(y_train_shuffled,axis=-1))\n",
    "i = 0\n",
    "for epoch in range(1000):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for start in range(0, len(x_train_shuffled), batch_size):\n",
    "        end = start + batch_size\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        inputs = x_train[start:end]\n",
    "\n",
    "        predicted = model(inputs)\n",
    "        actual = y_train[start:end]\n",
    "        # print(predicted.shape, actual.shape)\n",
    "        loss = mse(y_true=actual, y_pred=predicted)\n",
    "        if i==0:\n",
    "            draw_computation_graph(loss)\n",
    "        i+=1\n",
    "        # for param in model.parameters():\n",
    "        #     print(param.name, param.id)\n",
    "        # print(start, loss)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.data\n",
    "\n",
    "        optimizer.step(model)\n",
    "\n",
    "    print(epoch, epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b522a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[239.],\n",
       "       [255.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([[10,11,12], [11,12,13]])\n",
    "test = np.expand_dims(test, axis=-1)\n",
    "test = Tensor(test)\n",
    "print(test.shape)\n",
    "out = model(test)\n",
    "np.round(out.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7a37fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss=0.05099273701819455\n",
      "epoch 2: loss=13249.740727372679\n",
      "epoch 3: loss=14749.407963355394\n",
      "epoch 4: loss=15250.752942708701\n",
      "epoch 5: loss=15492.481381005673\n",
      "epoch 6: loss=15622.466162419676\n",
      "epoch 7: loss=15695.41284485934\n",
      "epoch 8: loss=15737.109844625535\n",
      "epoch 9: loss=15761.141082024957\n",
      "epoch 10: loss=15775.042803035232\n",
      "epoch 11: loss=15783.09837473718\n",
      "epoch 12: loss=15787.76982950266\n",
      "epoch 13: loss=15790.479695590704\n",
      "epoch 14: loss=15792.051856900975\n",
      "epoch 15: loss=15792.963996259414\n",
      "epoch 16: loss=15793.493200779878\n",
      "epoch 17: loss=15793.800228715167\n",
      "epoch 18: loss=15793.978352795524\n",
      "epoch 19: loss=15794.081690347573\n",
      "epoch 20: loss=15794.141639857253\n",
      "epoch 21: loss=15794.176417997249\n",
      "epoch 22: loss=15794.19659337004\n",
      "epoch 23: loss=15794.208297317833\n",
      "epoch 24: loss=15794.215086847486\n",
      "epoch 25: loss=15794.219025468892\n",
      "epoch 26: loss=15794.221310260786\n",
      "epoch 27: loss=15794.222635661932\n",
      "epoch 28: loss=15794.223404521004\n",
      "epoch 29: loss=15794.22385053153\n",
      "epoch 30: loss=15794.224109259143\n",
      "epoch 31: loss=15794.224259344925\n",
      "epoch 32: loss=15794.224346408413\n",
      "epoch 33: loss=15794.22439691318\n",
      "epoch 34: loss=15794.224426210523\n",
      "epoch 35: loss=15794.224443205623\n",
      "epoch 36: loss=15794.2244530643\n",
      "epoch 37: loss=15794.224458783203\n",
      "epoch 38: loss=15794.22446210072\n",
      "epoch 39: loss=15794.22446402514\n",
      "epoch 40: loss=15794.224465141517\n",
      "epoch 41: loss=15794.224465789084\n",
      "epoch 42: loss=15794.224466164736\n",
      "epoch 43: loss=15794.22446638269\n",
      "epoch 44: loss=15794.22446650907\n",
      "epoch 45: loss=15794.22446658242\n",
      "epoch 46: loss=15794.224466624943\n",
      "epoch 47: loss=15794.224466649632\n",
      "epoch 48: loss=15794.224466663925\n",
      "epoch 49: loss=15794.224466672225\n",
      "epoch 50: loss=15794.224466677038\n",
      "epoch 51: loss=15930.649908907928\n",
      "epoch 52: loss=15939.264734232322\n",
      "epoch 53: loss=15948.58238361161\n",
      "epoch 54: loss=15953.986863057577\n",
      "epoch 55: loss=15957.122304554452\n",
      "epoch 56: loss=15958.941586224675\n",
      "epoch 57: loss=15959.997270124293\n",
      "epoch 58: loss=15960.609883998388\n",
      "epoch 59: loss=15960.96539313605\n",
      "epoch 60: loss=15961.171703524114\n",
      "epoch 61: loss=15961.291431359525\n",
      "epoch 62: loss=15961.360913200611\n",
      "epoch 63: loss=15961.401235821391\n",
      "epoch 64: loss=15961.424636416967\n",
      "epoch 65: loss=15961.438216595921\n",
      "epoch 66: loss=15961.446097650123\n",
      "epoch 67: loss=15961.450671303874\n",
      "epoch 68: loss=15961.453325556908\n",
      "epoch 69: loss=15961.454865914031\n",
      "epoch 70: loss=15961.455759837934\n",
      "epoch 71: loss=15961.45627861366\n",
      "epoch 72: loss=15961.456579677691\n",
      "epoch 73: loss=15961.456754395815\n",
      "epoch 74: loss=15961.456855790992\n",
      "epoch 75: loss=15961.456914634151\n",
      "epoch 76: loss=15961.456948782956\n",
      "epoch 77: loss=15961.45696860073\n",
      "epoch 78: loss=15961.456980101626\n",
      "epoch 79: loss=15961.456986776077\n",
      "epoch 80: loss=15961.45699064951\n",
      "epoch 81: loss=15961.456992897345\n",
      "epoch 82: loss=15961.456994201864\n",
      "epoch 83: loss=15961.456994958962\n",
      "epoch 84: loss=15961.456995398261\n",
      "epoch 85: loss=15961.456995653254\n",
      "epoch 86: loss=15961.456995801243\n",
      "epoch 87: loss=15961.45699588706\n",
      "epoch 88: loss=15961.456995936904\n",
      "epoch 89: loss=15961.456995965844\n",
      "epoch 90: loss=15961.456995982642\n",
      "epoch 91: loss=15961.456995992376\n",
      "epoch 92: loss=15961.456995998022\n",
      "epoch 93: loss=15961.45699600127\n",
      "epoch 94: loss=15961.456996003191\n",
      "epoch 95: loss=15961.456996004295\n",
      "epoch 96: loss=15961.45699600493\n",
      "epoch 97: loss=15961.456996005327\n",
      "epoch 98: loss=15961.456996005525\n",
      "epoch 99: loss=15961.45699600566\n",
      "epoch 100: loss=15961.456996005729\n"
     ]
    }
   ],
   "source": [
    "def learning_rate_decay(epoch, optimizer):\n",
    "    if epoch != 0 and epoch % 50 == 0:\n",
    "        optimizer.learning_rate *= 0.99\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "004e4152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[777.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([[10,11,12]])\n",
    "test = np.expand_dims(test, axis=-1)\n",
    "np.round(net_conv_adam.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6b1651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>input_shape</th>\n",
       "      <th>output_shape</th>\n",
       "      <th>fc_layer_shape</th>\n",
       "      <th>kernels_shape</th>\n",
       "      <th>number_of_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN</td>\n",
       "      <td>(3, 1)</td>\n",
       "      <td>(3, 40)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RNN</td>\n",
       "      <td>(3, 40)</td>\n",
       "      <td>(40,)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FlattenLayer</td>\n",
       "      <td>(40,)</td>\n",
       "      <td>(1, 40)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FCLayer</td>\n",
       "      <td>(1, 40)</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>(40, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total number of params</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     type input_shape output_shape fc_layer_shape kernels_shape  number_of_params\n",
       "0                     RNN      (3, 1)      (3, 40)           None          None               880\n",
       "1                     RNN     (3, 40)        (40,)           None          None              2440\n",
       "2            FlattenLayer       (40,)      (1, 40)           None          None                 0\n",
       "3                 FCLayer     (1, 40)       (1, 1)        (40, 1)          None                41\n",
       "4  Total number of params                                                                    3361"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_conv_adam.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd36874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ml/Deep-Learning-Fundamentals/layers/rnn.py:226: RuntimeWarning: invalid value encountered in cast\n",
      "  dX[i, :] = dht_dxt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss=67342.6897958377\n",
      "epoch 2: loss=66715.57087041353\n",
      "epoch 3: loss=66715.57087041353\n",
      "epoch 4: loss=66715.57087041353\n",
      "epoch 5: loss=66715.57087041352\n",
      "epoch 6: loss=66715.57087041352\n",
      "epoch 7: loss=66715.57087041353\n",
      "epoch 8: loss=66715.57087041353\n",
      "epoch 9: loss=66715.57087041353\n",
      "epoch 10: loss=66715.57087041353\n",
      "epoch 11: loss=66715.57087041352\n",
      "epoch 12: loss=66715.57087041353\n",
      "epoch 13: loss=66715.57087041353\n",
      "epoch 14: loss=66715.57087041352\n",
      "epoch 15: loss=66715.57087041353\n",
      "epoch 16: loss=66715.57087041353\n",
      "epoch 17: loss=66715.57087041353\n",
      "epoch 18: loss=66715.57087041353\n",
      "epoch 19: loss=66715.57087041353\n",
      "epoch 20: loss=66715.57087041353\n",
      "epoch 21: loss=66715.57087041353\n",
      "epoch 22: loss=66715.57087041353\n",
      "epoch 23: loss=66715.57087041353\n",
      "epoch 24: loss=66715.57087041353\n",
      "epoch 25: loss=66715.57087041353\n",
      "epoch 26: loss=66715.57087041353\n",
      "epoch 27: loss=66715.57087041353\n",
      "epoch 28: loss=66715.57087041353\n",
      "epoch 29: loss=66715.57087041353\n",
      "epoch 30: loss=66715.57087041353\n",
      "epoch 31: loss=66715.57087041352\n",
      "epoch 32: loss=66715.57087041353\n",
      "epoch 33: loss=66715.57087041353\n",
      "epoch 34: loss=66715.57087041353\n",
      "epoch 35: loss=66715.57087041353\n",
      "epoch 36: loss=66715.57087041352\n",
      "epoch 37: loss=66715.57087041353\n",
      "epoch 38: loss=66715.57087041353\n",
      "epoch 39: loss=66715.57087041353\n",
      "epoch 40: loss=66715.57087041353\n",
      "epoch 41: loss=66715.57087041352\n",
      "epoch 42: loss=66715.57087041353\n",
      "epoch 43: loss=66715.57087041353\n",
      "epoch 44: loss=66715.57087041352\n",
      "epoch 45: loss=66715.57087041352\n",
      "epoch 46: loss=66715.57087041352\n",
      "epoch 47: loss=66715.57087041352\n",
      "epoch 48: loss=66715.57087041353\n",
      "epoch 49: loss=66715.57087041353\n",
      "epoch 50: loss=66715.57087041353\n",
      "epoch 51: loss=66715.57087041353\n",
      "epoch 52: loss=66715.57087041353\n",
      "epoch 53: loss=66715.57087041352\n",
      "epoch 54: loss=66715.57087041352\n",
      "epoch 55: loss=66715.57087041353\n",
      "epoch 56: loss=66715.57087041353\n",
      "epoch 57: loss=66715.57087041353\n",
      "epoch 58: loss=66715.57087041353\n",
      "epoch 59: loss=66715.57087041353\n",
      "epoch 60: loss=66715.57087041353\n",
      "epoch 61: loss=66715.57087041353\n",
      "epoch 62: loss=66715.57087041353\n",
      "epoch 63: loss=66715.57087041353\n",
      "epoch 64: loss=66715.57087041353\n",
      "epoch 65: loss=66715.57087041352\n",
      "epoch 66: loss=66715.57087041353\n",
      "epoch 67: loss=66715.57087041353\n",
      "epoch 68: loss=66715.57087041353\n",
      "epoch 69: loss=66715.57087041353\n",
      "epoch 70: loss=66715.57087041353\n",
      "epoch 71: loss=66715.57087041353\n",
      "epoch 72: loss=66715.57087041352\n",
      "epoch 73: loss=66715.57087041353\n",
      "epoch 74: loss=66715.57087041353\n",
      "epoch 75: loss=66715.57087041353\n",
      "epoch 76: loss=66715.57087041353\n",
      "epoch 77: loss=66715.57087041353\n",
      "epoch 78: loss=66715.57087041352\n",
      "epoch 79: loss=66715.57087041353\n",
      "epoch 80: loss=66715.57087041353\n",
      "epoch 81: loss=66715.57087041353\n",
      "epoch 82: loss=66715.57087041353\n",
      "epoch 83: loss=66715.57087041353\n",
      "epoch 84: loss=66715.57087041353\n",
      "epoch 85: loss=66715.57087041353\n",
      "epoch 86: loss=66715.57087041353\n",
      "epoch 87: loss=66715.57087041352\n",
      "epoch 88: loss=66715.57087041353\n",
      "epoch 89: loss=66715.57087041353\n",
      "epoch 90: loss=66715.57087041353\n",
      "epoch 91: loss=66715.57087041353\n",
      "epoch 92: loss=66715.57087041355\n",
      "epoch 93: loss=66715.57087041353\n",
      "epoch 94: loss=66715.57087041353\n",
      "epoch 95: loss=66715.57087041353\n",
      "epoch 96: loss=66715.57087041353\n",
      "epoch 97: loss=66715.57087041353\n",
      "epoch 98: loss=66715.57087041353\n",
      "epoch 99: loss=66715.57087041353\n",
      "epoch 100: loss=66715.57087041353\n"
     ]
    }
   ],
   "source": [
    "net_conv_sgd = Network()\n",
    "net_conv_sgd.set_optimizer(SGD())\n",
    "net_conv_sgd.add(RNN(hidden_size=20,output_size=30, return_sequences=False))\n",
    "net_conv_sgd.add(FlattenLayer())\n",
    "net_conv_sgd.add(FCLayer(output_dim=1))\n",
    "# net_conv_adam.add(SoftmaxLayer())\n",
    "\n",
    "# train on 1000 samples\n",
    "# we didn't implemented mini-batch GD\n",
    "net_conv_sgd.use(mse, mse_prime)\n",
    "net_conv_sgd.fit(x_train=x_train_shuffled, y_train=y_train_shuffled, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f56c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf15-gpu",
   "language": "python",
   "name": "tf15-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
